[返回](https://github.com/zouheq/CVPR-2020)
# 从野生图像中非监督学习 3D 可变形对象
作者： Shangzhe Wu; Christian Rupprecht; Andrea Vedaldi. University of Oxford


### 全文介绍

此文主要介绍一种由单一2D图像构建3D物体的方法，这个方法使用自动编码器(autoencoder)将输入的单一2D图像转换为深度(depth),反照率(albedo),观测点(viewpoint)和光照(illumination)四种信息。方法使用对称结构(symmetric strcture)的假设和处理满足非监督学习的要求。对于物体由于阴影而产生的不对称，此文通过对照明的推理来达到利用底层物体的对称性；对于可能(但不确定)的对象，此文通过和其他组件的端到端学习来预测概率对称图从而实现建模。结果表明在人脸，猫脸，汽车的3D构建上表现突出。

### 相关内容

- Structure  from  Motion (SfM)
- Shape from X.
- Category-specific reconstruction

### 具体方法
- 图像-几何 自动编码(Photo-geometric autoencoding)

即将输入的单一2D三通道图像通过函数<img src="https://latex.codecogs.com/gif.latex?\Omega">转换成深度(depth),反照率(albedo),观测点(viewpoint)和光照(illumination)四类图像信息。具体来说，训练神经网络，获得深度信息映射(depth map)：<img src="https://latex.codecogs.com/gif.latex?d: \Omega \rightarrow R_+">, 反照率映射(albedo map):<img src="https://latex.codecogs.com/gif.latex?a: \Omega \rightarrow R^3">, 全局光照方向：<img src="https://latex.codecogs.com/gif.latex?l \in S^2"> 和 观测点：<img src="https://latex.codecogs.com/gif.latex?w \in R^6">.
由于没有使用Ground Truth, 需要从生成的信息中重建原2D图片，来获得需要优化的目标损失函数。重建函数表示如下：
<img src="https://latex.codecogs.com/gif.latex?\hat{I} = \Pi (\Lambda(a,d,l),d,w)">
其中光照函数<img src="https://latex.codecogs.com/gif.latex?\Lambda">是观测点为<img src="https://latex.codecogs.com/gif.latex?w = 0">基于深度映射，光照方向和反照率的生成版本。<img src="https://latex.codecogs.com/gif.latex?\Pi">则是在此基础上针对特定观测点和深度映射信息的特定图像生成函数。优化目标则对应是：<img src="https://latex.codecogs.com/gif.latex?I \approx \hat{I}">.

- 概率对称物体(Probably symmetric objects)

为了确定3D重建物体的对称点。假设正面规范的深度信息和反照率都是沿着某一竖直平面对称的。首先水平方向(任意确定方向都可)翻转图像：<img src="https://latex.codecogs.com/gif.latex?[flip(a)]_{a,u,v} = a_{c,w-1-u,v}">.设置<img src="https://latex.codecogs.com/gif.latex?d \approx (flip({d}')"> 和 <img src="https://latex.codecogs.com/gif.latex?a \approx flip({a}')">.对于翻转后的图像有：
<img src="https://latex.codecogs.com/gif.latex?{\hat{I}}' = \Pi (\Lambda({a}',{d}',l),{d}',w)">,其中<img src="https://latex.codecogs.com/gif.latex?d \approx flip({d}')"> 和 <img src="https://latex.codecogs.com/gif.latex?a \approx flip({a}')">.
因此我们需要同时优化<img src="https://latex.codecogs.com/gif.latex?I \approx \hat{I}">和<img src="https://latex.codecogs.com/gif.latex?I \approx {\hat{I}}'">.原输入图像 <img src="https://latex.codecogs.com/gif.latex?I "> 和重建图像 <img src="https://latex.codecogs.com/gif.latex?\hat{I}'">的差异性通过以下函数描述：
其中<img src="https://latex.codecogs.com/gif.latex?l_{1,uv} = |\hat{I}_{uv} - I_{uv}|'">为两者间像素值在uv的L1距离,<img src="https://latex.codecogs.com/gif.latex?\sigma \in R_{+}^{W \times H}">为信心映射(confidence map).

总的学习目标为：
<img src="https://latex.codecogs.com/gif.latex?\varepsilon(\phi; I) = \L(\hat{I},I,\sigma) + \lambda_f \L({\hat{I}}',I,{\sigma}')">
其中<img src="https://latex.codecogs.com/gif.latex?\lambda_f = 0.5">是权重参数，<img src="https://latex.codecogs.com/gif.latex?(d,a,w,l,\sigma,{\sigma}') = \phi(I)">为神经网络的输出参数。


- 图像形成模型(Image formation model)

这部分具体设计图像的形成函数。(怎么说呢，大致看明白说的是什么，但是不知道为啥，图形3D建模方面的基础太弱)
- 感知损失(Perceptual loss)